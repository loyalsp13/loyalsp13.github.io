---
title: "[U] Week 3 (Day 2)"
date: 2021-02-02
categories: boostcamp AI Tech
---
# Lecture
Deeplearning Optimization

* Generalization = performance for test set (cross-validation)

* underfitting vs overfitting = high bias vs high variance (bias and variance tradeoff)

* Bootstrapping

    * Bagging : multiple models are being trained with different subsets of training data

    * Boosting : model is trained with different data sequentially

* matter of batch size : large batch converges to sharp minimizer, small batch converges to flat minimizer

    * flat minimizer has better performance for test set

* gradient descent methods

    * momentum : reflect previous gradient for update

    * nesterov accelerated gradient : reflect momentum + assumed weight updated with current gradient

    * Adagrad : control learning rate , update is getting smaller in long training period

    * Adadelta : no learning rate , momentum and adagrad with gradient squares + EMA of weight differences squares

    * RMSprop : momentum + Adagrad with gradient squares

    * Adam : leverages both past gradients and squared gradients , momentum + adaptive learning

* Regularization

    * early stopping , norm regularization , dropout , batch normalization

    * data augmentation : with small data size , traditional machine learning methods perform well better than deep learning

    * noise robustness

    * label smoothing : mix image and label with ratio

Convolution

* move kernels along input vector and multiply locally

* various kernels make different effects on images (blur, strenghthen edge)

* output_height = input_height - kernel_height + 1 (no padding, stride = 1)

* output channel size = number of previous kernel

* back propagation is also convolution

# Peer Session

* proper ways of cross-validation

    * never use test set for training

    * data should be distributed uniformly

    * apply voting, averaging for generated models output

* cv for time-series

    * temporal dependencies

    * order of data is important

    * build different models , train them with nested cross validation

# Assignment

* fill blanks of code which implementation of comparing various optimizers

    * Adam converges faster than other optimizers

