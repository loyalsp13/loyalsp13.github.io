---
title: "[U] Week 4 (Day 5)"
date: 2021-02-19
categories: boostcamp AI Tech
---
# Lecture

Natural language processing

* Self-supervised Pre-training Models

    * GPT-1

        * put 'extract' token to end of sentence -> transformer's input

        * connect two sentence with 'delim' token

    * BERT

        * bi-directional language model

        * Masked language model : replace words to 'mask' token

        * positional encoding is also learnable parameter

        * Segment embeddings : where word is belong (sentence A , sentence B)

        * Transfer learning : use pre-trained model to predict other tasks

        * SQuAD : Question Answering dataset

        * Big models have high accuracy

* Advanced Self-supervised Pre-training Models

    * GPT-2

        * Every NLP task can be solved with question answering

        * scrap web pages (Reddit , Wikipedia) -> Byte pair encoding

    * GPT-3

        * Few-shot learners (provide some examples) : increase of parameter size leads high increase of accuracy

    * ALBERT

        * larger models have some problems : memory limitation , training speed

        * factorized embedding prameterization : seperate parameter to reduce size
        
        * cross-layer parameter sharing : share parameters within encoder layers

    * ELECTRA

        * Discriminator of NLP task

    * Light-weight models

        * DistillBERT , TinyBERT

    * Fusing Knowledge Graph into Language Model

        * ERNIE , KagNET

* implementation

    * HuggingFace's Transformers library (bert, GPT-2)

# Assignment

Named Entity Recognition(NER) with Transformers library

# Peer Session

* review Seq2Seq, Beam Search, BLEU score

