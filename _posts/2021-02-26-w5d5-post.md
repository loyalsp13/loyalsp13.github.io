---
title: "[U] Week 5 (Day 5)"
date: 2021-02-26
categories: boostcamp AI Tech
---
# Lecture

Graph

* Inductive node embedding method

    * get encoder

* Graph Neural Network

    * each layer, aggregate neighbors' previous layer embeddings (Computation Graph)

    * aggregate function is shared between each nodes

    * calculate neighbors' mean and insert to neural network

    * loss = preserve each nodes' distance

    * can be used for downstream task -> End-to-End training

    * can get embedding of new nodes, not used nodes for training

* Graph Convolutional Network, GraphSAGE

* Graph Attention Network

    * neighbors' weight is also learnable parameter (self-attention)

    * attention vector = inner product with embedding pair -> weight

    * multi-head attention = train several attention vectors

* Graph representation training

    * represent whole graph as a vector

    * graph pooling = get graph embedding from node embeddings

* Over-smoothing problem

    * graph neural network gets deeper, node embeddings are too similar to each other

    * Jumping Knowledge Network = use whole layers' node embeddings

    * APPNP = only use neural network for first layer

* Graph augmentation

    * add edge between nodes with high similarity

* implementation

    * DGL library (GraphSAGE) + classification , Cora dataset

# Peer Session

* transductive method, recommendation system
