---
title: "[U] Week 3 (Day 4)"
date: 2021-02-04
categories: boostcamp AI Tech
---
# Lecture
Recurrent Neural Network

* sequence data : product of conditional probability , changeable length

* Markov model : Sequential model , current output is only affected by one previous timestep input (cannot represent long term context)

* Latent autoregressive model

    * basic RNN input : previous hidden states + current time's input data

    * BPTT (RNN's backpropagation) : product of partial derivatives , short term dependencies(vanishing gradient)

    * LSTM (Long Shot Term Memory)

        * previous cell state , previous hidden state , input

        * forget gate : decide information from input to throw away

        * input gate : decide information to store in the cell state

        * output gate : make output (new hidden state) using updated cell state

        * update cell : update cell state

    * GRU (Gated Recurrent Unit)

        * similar to LSTM , but don't have cell state

* Transformer : sequence transduction model based entirely on attention

    * input -> encoders (self-attention + feed forward) -> decoders (self-attention + encoder-decoder attention + feed forward) -> output

    * topmost encoder passes key and value decoders

    * self-attention (query , key , value)

        1. embedding input data

        2. scores = matmul(query, key.T) / sqrt(dim_key) -> masking -> softmax

        3. attention = weighted sum of scores and value

    * multi-head attention

        * split embedding vectors into n_head vectors

        * query, key, value are also splitted into n_head matrix

        * apply self-attention for each matrix

        * concatenate all matrix

    * decoder's self attention  : mask on next timesteps' data

    * encoder-decoder attention : key, value from encoder, queries from below decoders

    * vision transformer : split image to patch , apply position embedding
    
    * DALL-E : based on GPT-3 , create image based on description sentence

# Peer Session

* LSTM and moder CNN both solved vanishing gradient problems

* torch.LSTM(batch_first) : LSTM receives input tensor which leading dimension is batch_size

* torch has transformer class : https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html

# Assignment

* fill blanks of code which implementation of LSTM, transformer attention model

