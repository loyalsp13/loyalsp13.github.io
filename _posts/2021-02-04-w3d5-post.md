---
title: "[U] Week 3 (Day 5)"
date: 2021-02-05
categories: boostcamp AI Tech
---
# Lecture

Generative model

* generation (sampling) , density estimation (anomaly detection, explicit model) , unsupervised representation learning (feature learning)

* represent p(x) as parameter

    * with independent assumption : p(x1, ..., xn) = p(x1)p(x2)...p(xn)
    
        * n parameters needed

    * conditional independence : product of all conditional probability

        * full conditional independence : $2^n - 1$ parameters needed

        * only one conditional independence (Markov assumption) : 2n - 1 parameters needed

Auto-regressive Model

* NADE : Neural Autoregressive Density Estimator

    * compute the density of the given inputs

    * MNIST 28x28 pixels -> p(x) = p(x1, ... , x784) -> chain rule of conditional probability -> autoregressive model

    * need an ordering of all random variables

* Pixel RNN

    * use RNNS as an auto-regressive model

    * p(R | i) p(G | i, R) , p(B | i, R, G)

    * Row LSTM : accept above inputs

    * Diagonal BiLSTM : accept all previous inputs

Latent Variable Models

* Variational Auto-encoder

    * optimize variational distribution that best matches the posterior distribution(unknown, latent vector's probability) : minimize KL divergence between two distribution

    * input data's probability = ELBO + KL Divergence

        * minimize KL Divergence by maximizing ELBO

        * ELBO = reconstruction term - prior fitting term

    * hard to evaluate likelihood

    * prior fitting term must be differentiable

    * Adversial Auto-encoder

        * allows any arbitrary latent distribution

* Generative Adversarial Network

    * discriminator is mainly different part with VAE

    * discriminator vs generator (minimax game)

    * DCGAN, Info-GAN (condition) , Text2Image , Puzzle-GAN , CycleGAN (cycle consistency loss) , Star-GAN , Progressive-GAN

# Peer Session

* LSTM , Multi-head attention lecture review

