---
title: "[U] Week 4 (Day 1)"
date: 2021-02-15
categories: boostcamp AI Tech
---
# Lecture

Natural language processing

* Bag-of-Words

    * consturct vocabulary containig unique words

    * Encode words to one-hot vectors

    * Sentence can be represented as the sum of one-hot vectors

* NaiveBayes Classifier

    * MAP(maximum a posterior) method

    * conditional independence assumption

        * P(w1, w2, ... , wn | c)P(c) -> P(c)PI(P(wi | c))

* Word2Vec

    * dot(embedding weight, one-hot encoded input) -> only one column of weight selected = embedding vector

    * close embedding vectors have similar context

    * man -> woman = king -> queen = uncle -> aunt

    * Korea - Seoul + Tokyo = Japan

* GloVe

    * compute probability which word A and B exist in same sentence

    * minimize difference with probability and dot product of embedding A and B

* implementation

    * naive bayes classifier

        * konlpy : korean nlp package (tokenizer)

        * product of probability can be reduced to zero -> replace sum of log(P)

        * train(), inference(), set_priors(), set_likelihoods()

    * word2vec

        * konlpy : korean nlp package (tokenizer)

        * use torch Dataset, Dataloader

        * two word2vec model : CBOW, skip-gram

        * each model has two layer : embedding , output

        * train each model with nn.CrossEntropyLoss()

# Assignment

Get used to NLP's tokenization

# Peer Session

* review VAE, GAN Lectures

