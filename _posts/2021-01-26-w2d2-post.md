---
title: "[U] Week 2 (Day 2)"
date: 2021-01-26
categories: boostcamp AI Tech
---
# Lecture
Basic math

* differentiation

        import sympy as sym
        from sympy.abc import x

        sym.diff(mathematical expression with x, x)

* partial differentiation is used to handle vector

    * gradient vector

* gradient descent

    * subtract gradient to converge minimum value

    * iterate with multiplying learning rate

    * use whole data at one iteration

    * only can be used in convex function (linear regression)

* stochastic gradient descent

    * use mini-batches to calculate gradient and update

    * can be used in non-convex function

    * computation is more efficient than gradient descent

    * with proper mini-batch size, converge faster than gradient descent


# Peer Session

* find optimal feature values in linear regression with matrix form



