---
title: "[U] Week 4 (Day 4)"
date: 2021-02-18
categories: boostcamp AI Tech
---
# Lecture

Natural language processing

* Transformer

    * Similar lecture with Week 3 (Day 4)

    * scaled dot-produect attention : scale by the length of query / key vectors -> equalize softmax value to avoid gradinet vanishing

    * self-attention model can use GPU resource effectively

    * layer normalization : normalize within single instance -> second step : affine transform of each sequence vector

    * warm-up learning rate scheduler

    * masked self-attention

* implementation

    * multi-head attention model , masked multi-head attention model

# Assignment

Byte Pair Encoding

# Peer Session

* review Seq2Seq, Beam Search, BLEU score

