---
title: "[U] Week 8 (Day 4)"
date: 2021-03-18
categories: boostcamp AI Tech
---
# Lecture

Compression

mini code

* Floating-point operation

* (Hard)Max, Argmax, Softmax

Issues

* Fixed-point vs Floating-point (FPU)

    * floating-point covera much wider range of values

    * FPUs are much harder to implement

* Precision & accuracy

* Quantization

    * Affine quantization

    * quantize activation , weight

    * can't differentiate quantized value -> differentiate as original function

    * DoReFa-Net , Binarized Neural Networks

    * Dynamic quantization , post-training quantization < quantization-aware training

    * hardware-aware quantization

* Pruning

    * Inference speed , Regularization brings generalization

    * Information loss , granularity affects the efficiency of hardware accelerator design

    * remove weights which have close value to 0

    * pruning vs Dropout

    * retrain after pruning makes higher accuracy

    * unstructured pruning vs structured pruning (channel, kernel , layer)

    * iterative magnitude pruning (with rewinding)

    * lottery ticket hypothesis

* Distillation

    * Similar to Week 7 (Day 1)

    * affect of temperature on softmax output

    * zero-mean assumption

    * Feature distillation , Data-free knowledge distillation

toy code

* layer-wise histogram

* Teacher & student

Assignment

* Knowledge distillation